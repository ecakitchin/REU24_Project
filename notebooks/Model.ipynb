{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Model Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImprovedUNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder1 = self.conv_block(3, 64)\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.encoder4 = self.conv_block(256, 512)\n",
    "        self.encoder5 = self.conv_block(512, 1024)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = self.upconv_block(1024, 512)\n",
    "        self.decoder1 = self.conv_block(1024, 512)\n",
    "        self.upconv2 = self.upconv_block(512, 256)\n",
    "        self.decoder2 = self.conv_block(512, 256)\n",
    "        self.upconv3 = self.upconv_block(256, 128)\n",
    "        self.decoder3 = self.conv_block(256, 128)\n",
    "        self.upconv4 = self.upconv_block(128, 64)\n",
    "        self.decoder4 = self.conv_block(128, 64)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe1 = self.encoder1(x)\n",
    "        xe2 = self.encoder2(xe1)\n",
    "        xe3 = self.encoder3(xe2)\n",
    "        xe4 = self.encoder4(xe3)\n",
    "        xe5 = self.encoder5(xe4)\n",
    "\n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe5)\n",
    "        xu1 = torch.cat([xu1, xe4], dim=1)\n",
    "        xu1 = self.decoder1(xu1)\n",
    "\n",
    "        xu2 = self.upconv2(xu1)\n",
    "        xu2 = torch.cat([xu2, xe3], dim=1)\n",
    "        xu2 = self.decoder2(xu2)\n",
    "\n",
    "        xu3 = self.upconv3(xu2)\n",
    "        xu3 = torch.cat([xu3, xe2], dim=1)\n",
    "        xu3 = self.decoder3(xu3)\n",
    "\n",
    "        xu4 = self.upconv4(xu3)\n",
    "        xu4 = torch.cat([xu4, xe1], dim=1)\n",
    "        xu4 = self.decoder4(xu4)\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xu4)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None, subset_size=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.image_names = os.listdir(images_dir)[:subset_size]  # Use a subset of images if subset_size is provided\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        \n",
    "        mask_name = img_name.replace(\".png\", \"_mask.png\")  # Adjust this based on your mask naming convention\n",
    "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
    "        \n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)  # Read RGB image\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Read grayscale mask\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)  # Assuming single-channel mask\n",
    "        \n",
    "        return img, mask\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
